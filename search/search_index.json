{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Easily clean text with spaCy!</p>"},{"location":"#key-features","title":"Key Features","text":"<p>spacy-cleaner utilises <code>spaCy</code> <code>Language</code> models to replace, remove, and    mutate <code>spaCy</code> tokens. Cleaning actions available are:</p> <ul> <li>Remove/replace stopwords.</li> <li>Remove/replace punctuation.</li> <li>Remove/replace numbers.</li> <li>Remove/replace emails.</li> <li>Remove/replace URLs.</li> <li>Perform lemmatisation.</li> </ul> <p>See our docs for more information</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -U spacy-cleaner\n</code></pre> <p>or install with <code>Poetry</code></p> <pre><code>poetry add spacy-cleaner\n</code></pre>"},{"location":"#example","title":"\ud83d\udcd6 Example","text":"<p>spacy-cleaner can clean text written in any language <code>spaCy</code> has a model    for: <pre><code>import spacy\nfrom spacy_cleaner import processing, Cleaner\n\nmodel = spacy.load(\"en_core_web_sm\")\n</code></pre></p> <p>Class <code>Pipeline</code> allows for configurable cleaning of text using <code>spaCy</code>. The    <code>Pipeline</code> is initialised with a model and functions that transform <code>spaCy</code>    tokens:</p> <pre><code> cleaner = Cleaner( \n    model,\n    processing.remove_stopword_token,\n    processing.replace_punctuation_token,\n    processing.mutate_lemma_token,\n)\n</code></pre> <p>Next the <code>pipeline</code> can be called with the method <code>clean</code> to clean a list of    texts: <pre><code>texts = [\"Hello, my name is Cellan! I love to swim!\"]\n\ncleaner.clean(texts)\n</code></pre></p> About the method <code>clean</code>... <p>The method <code>clean</code> is a wrapper around the <code>spaCy</code> <code>Language</code> class method    <code>pipe</code>. Check the docs for more information:</p> <p>https://spacy.io/api/language#pipe</p> <p>Giving the output: <pre><code>['hello _IS_PUNCT_ Cellan _IS_PUNCT_ love swim _IS_PUNCT_']\n</code></pre></p>"},{"location":"#releases","title":"\ud83d\udcc8 Releases","text":"<p>You can see the list of available releases on the GitHub Releases page.</p> <p>We follow Semantic Versions specification.</p> <p>We use <code>Release Drafter</code>. As pull requests are merged, a draft release is kept up-to-date listing the changes, ready to publish when you\u2019re ready. With the categories option, you can categorize pull requests in release notes using labels.</p>"},{"location":"#list-of-labels-and-corresponding-titles","title":"List of labels and corresponding titles","text":"Label Title in Releases <code>enhancement</code>, <code>feature</code> \ud83d\ude80 Features <code>bug</code>, <code>refactoring</code>, <code>bugfix</code>, <code>fix</code> \ud83d\udd27 Fixes &amp; Refactoring <code>build</code>, <code>ci</code>, <code>testing</code> \ud83d\udce6 Build System &amp; CI/CD <code>breaking</code> \ud83d\udca5 Breaking Changes <code>documentation</code> \ud83d\udcdd Documentation <code>dependencies</code> \u2b06\ufe0f Dependencies updates <p>You can update it in <code>release-drafter.yml</code>.</p> <p>GitHub creates the <code>bug</code>, <code>enhancement</code>, and <code>documentation</code> labels for you. Dependabot creates the <code>dependencies</code> label. Create the remaining labels on the Issues tab of your GitHub repository, when you need them.</p>"},{"location":"#license","title":"\ud83d\udee1 License","text":"<p>This project is licensed under the terms of the <code>MIT</code> license. See LICENSE for more details.</p>"},{"location":"#citation","title":"\ud83d\udcc3 Citation","text":"<pre><code>@misc{spacy-cleaner,\n  author = {spacy-cleaner},\n  title = {Easily clean text with spaCy!},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/Ce11an/spacy-cleaner}}\n}\n</code></pre>"},{"location":"#credits","title":"\ud83d\ude80 Credits","text":"<p>This project was generated with <code>python-package-template</code></p> <p>This project was built using IntelliJ IDEA </p> <p></p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-code","title":"Contributing code","text":"<p>Check the Makefile for useful commands to help you get started. Run <code>make help</code> to see a list of all available commands.</p>"},{"location":"contributing/#dependencies","title":"Dependencies","text":"<p>We use Poetry to manage dependencies. Please  install Poetry before contributing code. We also utilise  pre-commit to manage pre-commit hooks. Please install pre-commit before contributing code.</p> <p>You can install all dependencies by running <code>make install</code> in the root directory  of this project.</p>"},{"location":"contributing/#code-style","title":"Code style","text":"<p>We use ruff to format and lint our code. Please  run <code>pre-commit run --all-files</code> or <code>make fmt</code> before submitting a pull  request to ensure your code is formatted correctly and passes all linting  checks.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We use pytest to test our code. Please  write tests for all new code you write. You can run all tests by running  <code>make test</code> in the root directory of this project.</p>"},{"location":"contributing/#type-checking","title":"Type checking","text":"<p>We use mypy to type check our code. Please run <code>make type-check</code> before  submitting a pull request to ensure your code is type checked correctly.</p>"},{"location":"contributing/#before-submitting-a-pull-request","title":"Before submitting a pull request","text":"<p>Before submitting a pull request, please ensure that you have run  <code>pre-commit run --all-files</code>, <code>make test</code>, and <code>make type-check</code> to ensure your  code is formatted correctly, passes all linting checks, and passes all tests.</p> <p>Please ensure you follow these steps:</p> <ol> <li>Fork the repository and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"getting-started/","title":"Getting started","text":"<p>You can install spacy-cleaner with <code>pip</code> or using <code>poetry</code>.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#with-pip","title":"with pip recommended","text":"<p>spacy-cleaner is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. If not, scroll down and expand the help box. Install with:</p> Latest3.x <pre><code>pip install spacy-cleaner\n</code></pre> <pre><code>pip install spacy-cleaner==\"3.*\" # (1)!\n</code></pre> <ol> <li>spacy-cleaner uses semantic versioning, which is why it's a     good idea to limit upgrades to the current major version.</li> </ol> <p>This will automatically install compatible versions of all dependencies: spaCy, spaCy Lookups, and tqdm. We always strive to support the latest versions, so there's no need to install those packages separately.</p>"},{"location":"getting-started/#with-poetry","title":"with Poetry","text":"<p>You can install spacy-cleaner using Poetry:</p> <pre><code>poetry add spacy-cleaner\n</code></pre>"},{"location":"help/","title":"Help","text":"<p>Do you like spacy-cleaner?</p> <p>Would you like to help spacy-cleaner, other users, and the author?</p> <p>Or would you like to get help with spacy-cleaner?</p> <p>There are very simple ways to help (several involve just one or two clicks).</p> <p>And there are several ways to get help too.</p>"},{"location":"help/#star-spacy-cleaner-on-github","title":"Star spacy-cleaner on GitHub","text":"<p>You can \"star\" spacy-cleaner on GitHub (clicking the star button at the top right) \u2b50\ufe0f</p> <p>By adding a star, other users will be able to find it more easily and see that it has been already useful for others.</p>"},{"location":"help/#watch-the-github-repository-for-releases","title":"Watch the GitHub repository for releases","text":"<p>You can \"watch\" spacy-cleaner in GitHub (clicking the \"watch\" button at the top right) \ud83d\udc40</p> <p>There you can select \"Releases only\".</p> <p>By doing it, you will receive notifications (in your email) whenever there's a new release (a new version) of spacy-cleaner with bug fixes and new features.</p>"},{"location":"help/#connect-with-the-author","title":"Connect with the author","text":"<p>You can connect with me, Cellan Hall, the author.</p> <p>You can:</p> <ul> <li>Follow me on GitHub.<ul> <li>See other Open Source projects I have created that could help you.</li> <li>Follow me to see when I create a new Open Source project.</li> </ul> </li> <li>Follow me on Twitter.<ul> <li>Tell me how you use spacy-cleaner.</li> </ul> </li> <li>Follow me on Linkedin.<ul> <li>Hear when I make announcements or release new tools.</li> </ul> </li> </ul>"},{"location":"help/#create-a-pull-request","title":"Create a Pull Request","text":"<p>You can contribute to the source code with Pull Requests.</p> <p>Thanks! \ud83d\ude80</p>"},{"location":"reference/cleaner/","title":"cleaner","text":"<p>Class <code>Cleaner</code> allows for configurable cleaning of text using <code>spaCy</code>.</p>"},{"location":"reference/cleaner/#spacy_cleaner.cleaners.Cleaner","title":"<code>Cleaner</code>","text":"<p>Cleans a sequence of texts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Language</code> <p>A <code>spaCy</code> model.</p> required <code>*processors</code> <code>Callable[[Token], Union[str, Token]]</code> <p>Callable token processors.</p> <code>()</code> Example <pre><code>import spacy\nfrom spacy_cleaner import Cleaner, processing\n\nmodel = spacy.blank(\"en\")\nmodel.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\nmodel.initialize()\n\ntexts = [\"Hello, my name is Cellan! I love to swim!\"]\n\ncleaner = Cleaner(\n    model,\n    processing.remove_stopword_token,\n    processing.replace_punctuation_token,\n    processing.mutate_lemma_token,\n)\ncleaner.clean(texts)\n['hello _IS_PUNCT_ Cellan _IS_PUNCT_ love swim _IS_PUNCT_']\n</code></pre> Source code in <code>spacy_cleaner/cleaners.py</code> <pre><code>class Cleaner:\n    \"\"\"Cleans a sequence of texts.\n\n    Args:\n        model: A `spaCy` model.\n        *processors: Callable token processors.\n\n    Example:\n        ```python\n        import spacy\n        from spacy_cleaner import Cleaner, processing\n\n        model = spacy.blank(\"en\")\n        model.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n        model.initialize()\n\n        texts = [\"Hello, my name is Cellan! I love to swim!\"]\n\n        cleaner = Cleaner(\n            model,\n            processing.remove_stopword_token,\n            processing.replace_punctuation_token,\n            processing.mutate_lemma_token,\n        )\n        cleaner.clean(texts)\n        ['hello _IS_PUNCT_ Cellan _IS_PUNCT_ love swim _IS_PUNCT_']\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: spacy.Language,\n        *processors: Callable[[tokens.Token], Union[str, tokens.Token]],\n    ) -&gt; None:\n        self.model = model\n        self.processors = processors\n\n    # noinspection PyTypeChecker,PyDefaultArgumentdd,PyDefaultArgument\n    def clean(  # noqa: PLR0913\n        self,\n        texts: Union[\n            Iterable[Union[str, tokens.Doc]],\n            Iterable[Tuple[Union[str, tokens.Doc], _AnyContext]],\n        ],\n        *,\n        as_tuples: bool = False,\n        batch_size: Optional[int] = None,\n        disable: Iterable[str] = util.SimpleFrozenList(),\n        component_cfg: Optional[Dict[str, Dict[str, Any]]] = None,\n        n_process: int = 1,\n    ) -&gt; List[str]:\n        \"\"\"Clean a stream of texts.\n\n        Args:\n            texts: A sequence of texts or docs to process.\n            as_tuples: If set to True, inputs should be a sequence of\n                (text, context) tuples. Output will then be a sequence of\n                (doc, context) tuples.\n            batch_size: The number of texts to buffer.\n            disable: The pipeline components to disable.\n            component_cfg: An optional dictionary with extra keyword arguments\n                for specific components.\n            n_process: Number of processors to process texts. If `-1`, set\n                `multiprocessing.cpu_count()`.\n\n        Returns:\n              A list of cleaned strings in the order of the original text.\n\n        References:\n            https://spacy.io/api/language#pipe\n        \"\"\"\n        return [\n            helpers.clean_doc(doc, *self.processors)\n            for doc in tqdm.tqdm(\n                self.model.pipe(  # type: ignore[call-overload]\n                    texts,\n                    as_tuples=as_tuples,\n                    batch_size=batch_size,\n                    disable=disable,\n                    component_cfg=component_cfg,\n                    n_process=n_process,\n                ),\n                desc=\"Cleaning Progress\",\n                total=len(texts),  # type: ignore[arg-type]\n            )\n        ]\n</code></pre>"},{"location":"reference/cleaner/#spacy_cleaner.cleaners.Cleaner.clean","title":"<code>clean(texts, *, as_tuples=False, batch_size=None, disable=util.SimpleFrozenList(), component_cfg=None, n_process=1)</code>","text":"<p>Clean a stream of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Union[Iterable[Union[str, Doc]], Iterable[Tuple[Union[str, Doc], _AnyContext]]]</code> <p>A sequence of texts or docs to process.</p> required <code>as_tuples</code> <code>bool</code> <p>If set to True, inputs should be a sequence of (text, context) tuples. Output will then be a sequence of (doc, context) tuples.</p> <code>False</code> <code>batch_size</code> <code>Optional[int]</code> <p>The number of texts to buffer.</p> <code>None</code> <code>disable</code> <code>Iterable[str]</code> <p>The pipeline components to disable.</p> <code>SimpleFrozenList()</code> <code>component_cfg</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>An optional dictionary with extra keyword arguments for specific components.</p> <code>None</code> <code>n_process</code> <code>int</code> <p>Number of processors to process texts. If <code>-1</code>, set <code>multiprocessing.cpu_count()</code>.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of cleaned strings in the order of the original text.</p> References <p>https://spacy.io/api/language#pipe</p> Source code in <code>spacy_cleaner/cleaners.py</code> <pre><code>def clean(  # noqa: PLR0913\n    self,\n    texts: Union[\n        Iterable[Union[str, tokens.Doc]],\n        Iterable[Tuple[Union[str, tokens.Doc], _AnyContext]],\n    ],\n    *,\n    as_tuples: bool = False,\n    batch_size: Optional[int] = None,\n    disable: Iterable[str] = util.SimpleFrozenList(),\n    component_cfg: Optional[Dict[str, Dict[str, Any]]] = None,\n    n_process: int = 1,\n) -&gt; List[str]:\n    \"\"\"Clean a stream of texts.\n\n    Args:\n        texts: A sequence of texts or docs to process.\n        as_tuples: If set to True, inputs should be a sequence of\n            (text, context) tuples. Output will then be a sequence of\n            (doc, context) tuples.\n        batch_size: The number of texts to buffer.\n        disable: The pipeline components to disable.\n        component_cfg: An optional dictionary with extra keyword arguments\n            for specific components.\n        n_process: Number of processors to process texts. If `-1`, set\n            `multiprocessing.cpu_count()`.\n\n    Returns:\n          A list of cleaned strings in the order of the original text.\n\n    References:\n        https://spacy.io/api/language#pipe\n    \"\"\"\n    return [\n        helpers.clean_doc(doc, *self.processors)\n        for doc in tqdm.tqdm(\n            self.model.pipe(  # type: ignore[call-overload]\n                texts,\n                as_tuples=as_tuples,\n                batch_size=batch_size,\n                disable=disable,\n                component_cfg=component_cfg,\n                n_process=n_process,\n            ),\n            desc=\"Cleaning Progress\",\n            total=len(texts),  # type: ignore[arg-type]\n        )\n    ]\n</code></pre>"},{"location":"reference/processing/evaluators/","title":"evaluators","text":"<p>Evaluate <code>spaCy</code> tokens.</p> <p>This module contains classes that assist with evaluating <code>spaCy</code> tokens.</p> A typical usage example <p><pre><code>import spacy\nfrom spacy_cleaner.processing import evaluators\n\nnlp = spacy.load(\"en_core_web_md\")\ndoc = nlp(\"and\")\ntok = doc[0]\n\nevaluator = evaluators.StopwordsEvaluator()\nevaluator.evaluate(tok)\n</code></pre> Calling evaluate returns <code>True</code> as <code>and</code> is a stopword.</p>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.EmailEvaluator","title":"<code>EmailEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>Evaluates emails.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class EmailEvaluator(Evaluator):\n    \"\"\"Evaluates emails.\"\"\"\n\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"If the given token is like an email.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n            `True` if the token is like an email. `False` if not.\n        \"\"\"\n        return tok.like_email\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.EmailEvaluator.evaluate","title":"<code>evaluate(tok)</code>","text":"<p>If the given token is like an email.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the token is like an email. <code>False</code> if not.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>def evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"If the given token is like an email.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n        `True` if the token is like an email. `False` if not.\n    \"\"\"\n    return tok.like_email\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.Evaluator","title":"<code>Evaluator</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for evaluators.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class Evaluator(abc.ABC):\n    \"\"\"Base class for evaluators.\"\"\"\n\n    @abc.abstractmethod\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"Evaluates a `spaCy` token.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n           Whether the token is evaluated to `True` or `False`.\n        \"\"\"\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.Evaluator.evaluate","title":"<code>evaluate(tok)</code>  <code>abstractmethod</code>","text":"<p>Evaluates a <code>spaCy</code> token.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the token is evaluated to <code>True</code> or <code>False</code>.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>@abc.abstractmethod\ndef evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"Evaluates a `spaCy` token.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n       Whether the token is evaluated to `True` or `False`.\n    \"\"\"\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.NumberEvaluator","title":"<code>NumberEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>Evaluates Numbers.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class NumberEvaluator(Evaluator):\n    \"\"\"Evaluates Numbers.\"\"\"\n\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"If the given token is like a number.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n            `True` if the token is like a number. `False` if not.\n        \"\"\"\n        return tok.like_num\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.NumberEvaluator.evaluate","title":"<code>evaluate(tok)</code>","text":"<p>If the given token is like a number.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the token is like a number. <code>False</code> if not.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>def evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"If the given token is like a number.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n        `True` if the token is like a number. `False` if not.\n    \"\"\"\n    return tok.like_num\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.PunctuationEvaluator","title":"<code>PunctuationEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>Evaluates emails.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class PunctuationEvaluator(Evaluator):\n    \"\"\"Evaluates emails.\"\"\"\n\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"If the given token is like an email.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n            `True` if the token is punctuation. `False` if not.\n        \"\"\"\n        return tok.is_punct\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.PunctuationEvaluator.evaluate","title":"<code>evaluate(tok)</code>","text":"<p>If the given token is like an email.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the token is punctuation. <code>False</code> if not.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>def evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"If the given token is like an email.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n        `True` if the token is punctuation. `False` if not.\n    \"\"\"\n    return tok.is_punct\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.StopwordsEvaluator","title":"<code>StopwordsEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>Evaluates stopwords.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class StopwordsEvaluator(Evaluator):\n    \"\"\"Evaluates stopwords.\"\"\"\n\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"If the given token is a stopword.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n            `True` if the token is a stopword. `False` if not.\n        \"\"\"\n        return tok.is_stop\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.StopwordsEvaluator.evaluate","title":"<code>evaluate(tok)</code>","text":"<p>If the given token is a stopword.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the token is a stopword. <code>False</code> if not.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>def evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"If the given token is a stopword.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n        `True` if the token is a stopword. `False` if not.\n    \"\"\"\n    return tok.is_stop\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.URLEvaluator","title":"<code>URLEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>Evaluates URLs.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>class URLEvaluator(Evaluator):\n    \"\"\"Evaluates URLs.\"\"\"\n\n    def evaluate(self, tok: tokens.Token) -&gt; bool:\n        \"\"\"If the given token is like a URL.\n\n        Args:\n            tok: Token to evaluate.\n\n        Returns:\n            `True` if the token is like a URL. `False` if not.\n        \"\"\"\n        return tok.like_url\n</code></pre>"},{"location":"reference/processing/evaluators/#spacy_cleaner.processing.evaluators.URLEvaluator.evaluate","title":"<code>evaluate(tok)</code>","text":"<p>If the given token is like a URL.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>Token to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the token is like a URL. <code>False</code> if not.</p> Source code in <code>spacy_cleaner/processing/evaluators.py</code> <pre><code>def evaluate(self, tok: tokens.Token) -&gt; bool:\n    \"\"\"If the given token is like a URL.\n\n    Args:\n        tok: Token to evaluate.\n\n    Returns:\n        `True` if the token is like a URL. `False` if not.\n    \"\"\"\n    return tok.like_url\n</code></pre>"},{"location":"reference/processing/helpers/","title":"helpers","text":"<p>Processing helper functions.</p>"},{"location":"reference/processing/helpers/#spacy_cleaner.processing.helpers.clean_doc","title":"<code>clean_doc(doc, *processors)</code>","text":"<p>Cleans a spaCy document and returns a cleaned string.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>spaCy document to be cleaned.</p> required <code>*processors</code> <code>Callable[[Token], Union[str, Token]]</code> <p>Callable token processors.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>A string of the cleaned text.</p> Source code in <code>spacy_cleaner/processing/helpers.py</code> <pre><code>def clean_doc(\n    doc: tokens.Doc,\n    *processors: Callable[[tokens.Token], Union[str, tokens.Token]],\n) -&gt; str:\n    \"\"\"Cleans a spaCy document and returns a cleaned string.\n\n    Args:\n        doc: spaCy document to be cleaned.\n        *processors: Callable token processors.\n\n    Returns:\n        A string of the cleaned text.\n    \"\"\"\n    s = \" \".join([token_pipe(tok, *processors) for tok in doc])\n    return replace_multi_whitespace(s)\n</code></pre>"},{"location":"reference/processing/helpers/#spacy_cleaner.processing.helpers.replace_multi_whitespace","title":"<code>replace_multi_whitespace(s, replace=' ')</code>","text":"<p>Replace multiple whitespace characters with a single space.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to be replaced.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>' '</code> <p>Returns:</p> Type Description <code>str</code> <p>A string with all the whitespace replaced with a single space.</p> Source code in <code>spacy_cleaner/processing/helpers.py</code> <pre><code>def replace_multi_whitespace(s: str, replace: str = \" \") -&gt; str:\n    \"\"\"Replace multiple whitespace characters with a single space.\n\n    Args:\n      s: The string to be replaced.\n      replace: The replacement string.\n\n    Returns:\n      A string with all the whitespace replaced with a single space.\n    \"\"\"\n    return re.sub(r\"\\s\\s+\", replace, s, flags=re.UNICODE).strip()\n</code></pre>"},{"location":"reference/processing/helpers/#spacy_cleaner.processing.helpers.token_pipe","title":"<code>token_pipe(tok, *processors)</code>","text":"<p>Applies a series of processors to a token until it becomes a string.</p> <p>It takes a token, and applies a series of functions to it, until one of     the functions returns a string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>The token to be transformed,</p> required <code>*processors</code> <code>Callable[[Token], Union[str, Token]]</code> <p>Callable token processors.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>A string of the token after being processed.</p> Source code in <code>spacy_cleaner/processing/helpers.py</code> <pre><code>def token_pipe(\n    tok: tokens.Token,\n    *processors: Callable[[tokens.Token], Union[str, tokens.Token]],\n) -&gt; str:\n    \"\"\"Applies a series of processors to a token until it becomes a string.\n\n    It takes a token, and applies a series of functions to it, until one of\n        the functions returns a string.\n\n    Args:\n        tok: The token to be transformed,\n        *processors: Callable token processors.\n\n    Returns:\n        A string of the token after being processed.\n    \"\"\"\n    for processor in processors:\n        tok = processor(tok)  # type: ignore[assignment]\n        if isinstance(tok, str):\n            return str(tok)\n    return str(tok)\n</code></pre>"},{"location":"reference/processing/mutators/","title":"mutators","text":"<p>Mutate <code>spaCy</code> tokens.</p> <p>This module contains functions that assist with mutating <code>spaCy</code> tokens.</p> A typical usage example <p><pre><code>import spacy\nfrom spacy_cleaner import processing\n\nnlp = spacy.load(\"en_core_web_md\")\ndoc = nlp(\"swimming\")\ntok = doc[0]\n\nprocessing.mutate_lemma_toke(tok)\n</code></pre> The lemma of <code>swimming</code> is <code>swim</code>.</p>"},{"location":"reference/processing/mutators/#spacy_cleaner.processing.mutators.mutate_lemma_token","title":"<code>mutate_lemma_token(tok)</code>","text":"<p>Mutate a token to its lemma.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>tokens.Token</p> required <p>Returns:</p> Type Description <code>str</code> <p>The lemma of the token.</p> Source code in <code>spacy_cleaner/processing/mutators.py</code> <pre><code>def mutate_lemma_token(tok: tokens.Token) -&gt; str:\n    \"\"\"Mutate a token to its lemma.\n\n    Args:\n        tok: tokens.Token\n\n    Returns:\n        The lemma of the token.\n    \"\"\"\n    return tok.lemma_\n</code></pre>"},{"location":"reference/processing/removers/","title":"removers","text":"<p>Remove <code>spaCy</code> tokens.</p> <p>This module contains functions that assist with removing <code>spaCy</code> tokens.</p> A typical usage example <p><pre><code>import spacy\nfrom spacy_cleaner import processing\n\nnlp = spacy.load(\"en_core_web_md\")\ndoc = nlp(\"and\")\ntok = doc[0]\n\nprocessing.remove_stopword_token(tok)\n</code></pre> <code>and</code> is a stopword so an empty string is returned.</p>"},{"location":"reference/processing/removers/#spacy_cleaner.processing.removers.remove_email_token","title":"<code>remove_email_token(tok)</code>","text":"<p>If the token is like an email, replace it with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>An empty string or the original token.</p> Source code in <code>spacy_cleaner/processing/removers.py</code> <pre><code>def remove_email_token(tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like an email, replace it with an empty string.\n\n    Args:\n      tok: A `spaCy` token.\n\n    Returns:\n      An empty string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.EmailEvaluator(), replace=\"\"\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/removers/#spacy_cleaner.processing.removers.remove_number_token","title":"<code>remove_number_token(tok)</code>","text":"<p>If the token is like a number, replace it with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>An empty string or the original token.</p> Source code in <code>spacy_cleaner/processing/removers.py</code> <pre><code>def remove_number_token(tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like a number, replace it with an empty string.\n\n    Args:\n      tok: A `spaCy` token.\n\n    Returns:\n      An empty string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.NumberEvaluator(), replace=\"\"\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/removers/#spacy_cleaner.processing.removers.remove_punctuation_token","title":"<code>remove_punctuation_token(tok)</code>","text":"<p>If the token is punctuation, replace it with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>An empty string or the original token.</p> Source code in <code>spacy_cleaner/processing/removers.py</code> <pre><code>def remove_punctuation_token(tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is punctuation, replace it with an empty string.\n\n    Args:\n      tok: A `spaCy` token.\n\n    Returns:\n      An empty string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.PunctuationEvaluator(), replace=\"\"\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/removers/#spacy_cleaner.processing.removers.remove_stopword_token","title":"<code>remove_stopword_token(tok)</code>","text":"<p>If the token is a stopword, replace it with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>An empty string or the original token.</p> Source code in <code>spacy_cleaner/processing/removers.py</code> <pre><code>def remove_stopword_token(tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is a stopword, replace it with an empty string.\n\n    Args:\n      tok: A `spaCy` token.\n\n    Returns:\n      An empty string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.StopwordsEvaluator(), replace=\"\"\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/removers/#spacy_cleaner.processing.removers.remove_url_token","title":"<code>remove_url_token(tok)</code>","text":"<p>If the token is like a URL, replace it with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>An empty string or the original token.</p> Source code in <code>spacy_cleaner/processing/removers.py</code> <pre><code>def remove_url_token(tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like a URL, replace it with an empty string.\n\n    Args:\n      tok: A `spaCy` token.\n\n    Returns:\n      An empty string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.URLEvaluator(), replace=\"\"\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/replacers/","title":"replacers","text":"<p>Replace <code>spaCy</code> tokens.</p> <p>This module contains functions that assist with replace <code>spaCy</code> tokens.</p> A typical usage example <p><pre><code>import spacy\nfrom spacy_cleaner import processing\n\nnlp = spacy.load(\"en_core_web_md\")\ndoc = nlp(\",\")\ntok = doc[0]\n\nprocessing.replace_punctuation_token(tok)\n</code></pre> <code>,</code> is replaced with <code>_IS_PUNCT_</code>.</p>"},{"location":"reference/processing/replacers/#spacy_cleaner.processing.replacers.replace_email_token","title":"<code>replace_email_token(tok, replace='_LIKE_EMAIL_')</code>","text":"<p>If the token is like an email, replace it with the string <code>_LIKE_EMAIL_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>'_LIKE_EMAIL_'</code> <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>The replacement string or the original token.</p> Source code in <code>spacy_cleaner/processing/replacers.py</code> <pre><code>def replace_email_token(\n    tok: tokens.Token, replace: str = \"_LIKE_EMAIL_\"\n) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like an email, replace it with the string `_LIKE_EMAIL_`.\n\n    Args:\n      tok: A `spaCy` token.\n      replace: The replacement string.\n\n    Returns:\n      The replacement string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.EmailEvaluator(), replace\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/replacers/#spacy_cleaner.processing.replacers.replace_number_token","title":"<code>replace_number_token(tok, replace='_LIKE_NUM_')</code>","text":"<p>If the token is like a number, replace it with the string <code>_LIKE_NUM_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>'_LIKE_NUM_'</code> <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>The replacement string or the original token.</p> Source code in <code>spacy_cleaner/processing/replacers.py</code> <pre><code>def replace_number_token(\n    tok: tokens.Token, replace: str = \"_LIKE_NUM_\"\n) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like a number, replace it with the string `_LIKE_NUM_`.\n\n    Args:\n      tok: A `spaCy` token.\n      replace: The replacement string.\n\n    Returns:\n      The replacement string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.NumberEvaluator(), replace\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/replacers/#spacy_cleaner.processing.replacers.replace_punctuation_token","title":"<code>replace_punctuation_token(tok, replace='_IS_PUNCT_')</code>","text":"<p>If the token is punctuation, replace it with the string <code>_IS_PUNCT_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>'_IS_PUNCT_'</code> <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>The replacement string or the original token.</p> Source code in <code>spacy_cleaner/processing/replacers.py</code> <pre><code>def replace_punctuation_token(\n    tok: tokens.Token, replace: str = \"_IS_PUNCT_\"\n) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is punctuation, replace it with the string `_IS_PUNCT_`.\n\n    Args:\n      tok: A `spaCy` token.\n      replace: The replacement string.\n\n    Returns:\n      The replacement string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.PunctuationEvaluator(), replace\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/replacers/#spacy_cleaner.processing.replacers.replace_stopword_token","title":"<code>replace_stopword_token(tok, replace='_IS_STOP_')</code>","text":"<p>If the token is a stopword, replace it with the string <code>_IS_STOP_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>'_IS_STOP_'</code> <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>The replacement string or the original token.</p> Source code in <code>spacy_cleaner/processing/replacers.py</code> <pre><code>def replace_stopword_token(\n    tok: tokens.Token, replace: str = \"_IS_STOP_\"\n) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is a stopword, replace it with the string `_IS_STOP_`.\n\n    Args:\n      tok: A `spaCy` token.\n      replace: The replacement string.\n\n    Returns:\n      The replacement string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.StopwordsEvaluator(), replace\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/replacers/#spacy_cleaner.processing.replacers.replace_url_token","title":"<code>replace_url_token(tok, replace='_LIKE_URL_')</code>","text":"<p>If the token is like a URL, replace it with the string <code>_LIKE_URL_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>A <code>spaCy</code> token.</p> required <code>replace</code> <code>str</code> <p>The replacement string.</p> <code>'_LIKE_URL_'</code> <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>The replacement string or the original token.</p> Source code in <code>spacy_cleaner/processing/replacers.py</code> <pre><code>def replace_url_token(\n    tok: tokens.Token, replace: str = \"_LIKE_URL_\"\n) -&gt; Union[str, tokens.Token]:\n    \"\"\"If the token is like a URL, replace it with the string `_LIKE_URL_`.\n\n    Args:\n      tok: A `spaCy` token.\n      replace: The replacement string.\n\n    Returns:\n      The replacement string or the original token.\n    \"\"\"\n    return transformers.Transformer(\n        evaluators.URLEvaluator(), replace\n    ).transform(tok)\n</code></pre>"},{"location":"reference/processing/transformers/","title":"transformers","text":"<p>Class <code>TokenTransformer</code> allows for transformation of <code>spaCy</code> tokens.</p>"},{"location":"reference/processing/transformers/#spacy_cleaner.processing.transformers.Transformer","title":"<code>Transformer</code>","text":"<p>Transforms a token using the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>Evaluator</code> <p>Evaluates if the token should be processed or not.</p> required <code>replace</code> <code>str</code> <p>Replaces token based on the token evaluation.</p> required Example <pre><code>from spacy_cleaner.processing.evaluators import StopwordsEvaluator\n\ntransformer = Transformer(StopwordsEvaluator(), replace=\"\")\ntransformer.transform(tok)\n</code></pre> Source code in <code>spacy_cleaner/processing/transformers.py</code> <pre><code>class Transformer:\n    \"\"\"Transforms a token using the evaluator.\n\n    Args:\n        evaluator: Evaluates if the token should be processed or not.\n        replace: Replaces token based on the token evaluation.\n\n    Example:\n        ```python\n        from spacy_cleaner.processing.evaluators import StopwordsEvaluator\n\n        transformer = Transformer(StopwordsEvaluator(), replace=\"\")\n        transformer.transform(tok)\n        ```\n    \"\"\"\n\n    def __init__(self, evaluator: evaluators.Evaluator, replace: str) -&gt; None:\n        self.evaluator = evaluator\n        self.replace = replace\n\n    def transform(self, tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n        \"\"\"Processes a token using the evaluator.\n\n        Args:\n            tok: The token to be evaluated.\n\n        Returns:\n            A string or token depending on evaluation.\n        \"\"\"\n        return self.replace if self.evaluator.evaluate(tok) else tok\n</code></pre>"},{"location":"reference/processing/transformers/#spacy_cleaner.processing.transformers.Transformer.transform","title":"<code>transform(tok)</code>","text":"<p>Processes a token using the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>tok</code> <code>Token</code> <p>The token to be evaluated.</p> required <p>Returns:</p> Type Description <code>Union[str, Token]</code> <p>A string or token depending on evaluation.</p> Source code in <code>spacy_cleaner/processing/transformers.py</code> <pre><code>def transform(self, tok: tokens.Token) -&gt; Union[str, tokens.Token]:\n    \"\"\"Processes a token using the evaluator.\n\n    Args:\n        tok: The token to be evaluated.\n\n    Returns:\n        A string or token depending on evaluation.\n    \"\"\"\n    return self.replace if self.evaluator.evaluate(tok) else tok\n</code></pre>"}]}